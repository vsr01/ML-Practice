{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430de2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "import wandb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "def model_with_gridsearch(\n",
    "    data, \n",
    "    model, \n",
    "    param_grid, \n",
    "    pipeline_steps=None, \n",
    "    target='TenYearCHD',\n",
    "    scoring='f1', \n",
    "    cv=5,\n",
    "    verbose=1,\n",
    "    use_wandb=False,  # <--- New optional parameter\n",
    "    wandb_project=\"ml_experiments\"  # <--- Default project name\n",
    "):\n",
    "    \"\"\"\n",
    "    Generalized pipeline with GridSearchCV for any model, scoring, and preprocessing steps,\n",
    "    now optionally logging results to Weights & Biases (wandb).\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    best_pipeline : sklearn Pipeline\n",
    "    grid : GridSearchCV object\n",
    "    \"\"\"\n",
    "    df = data.copy()\n",
    "    X = df.drop(columns=target)\n",
    "    y = df[target]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "    if pipeline_steps is None:\n",
    "        pipeline_steps = [\n",
    "            ('outlier_handler', OutlierCapperOrRemover(strategy='cap', factor=1.5)),\n",
    "            ('imputer', KNNImputer(n_neighbors=5)),\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('smote', SMOTE(random_state=42))\n",
    "        ]\n",
    "\n",
    "    full_pipeline = Pipeline(pipeline_steps + [('model', model)])\n",
    "\n",
    "    grid = GridSearchCV(\n",
    "        estimator=full_pipeline,\n",
    "        param_grid=param_grid,\n",
    "        scoring=scoring,\n",
    "        cv=cv,\n",
    "        n_jobs=-1,\n",
    "        refit=scoring if isinstance(scoring, str) else 'f1',\n",
    "        verbose=verbose\n",
    "    )\n",
    "\n",
    "    grid.fit(X_train, y_train)\n",
    "\n",
    "    best_pipeline = grid.best_estimator_\n",
    "\n",
    "    print(\"\\nBest hyperparameters found:\", grid.best_params_)\n",
    "    model_scoring(best_pipeline, X_train, X_test, y_train, y_test)\n",
    "\n",
    "    # ðŸš€ WandB Logging\n",
    "    if use_wandb:\n",
    "        wandb.init(project=wandb_project, name=type(model).__name__)\n",
    "        \n",
    "        # Best model metrics\n",
    "        y_pred = best_pipeline.predict(X_test)\n",
    "        acc = accuracy_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        prec = precision_score(y_test, y_pred)\n",
    "        rec = recall_score(y_test, y_pred)\n",
    "\n",
    "        wandb.config.update({\n",
    "            \"model\": type(model).__name__,\n",
    "            \"preprocessing\": [step[0] for step in pipeline_steps],\n",
    "            **grid.best_params_  # Add best hyperparameters\n",
    "        })\n",
    "\n",
    "        wandb.log({\n",
    "            \"accuracy\": acc,\n",
    "            \"f1_score\": f1,\n",
    "            \"precision\": prec,\n",
    "            \"recall\": rec\n",
    "        })\n",
    "\n",
    "        wandb.finish()\n",
    "\n",
    "    return best_pipeline, grid\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ########################################################\n",
    "\n",
    "\n",
    "\n",
    "best_pipeline, search = model_with_gridsearch(\n",
    "    data=df,\n",
    "    model=LogisticRegression(),\n",
    "    param_grid=param_grid_lr,\n",
    "    use_wandb=True,  # <--- Turn on wandb logging\n",
    "    wandb_project=\"logistic_regression_colab\"\n",
    ")\n",
    "\n",
    "    #########################################################\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "param_grid_lr = {\n",
    "    'model__C': [0.01, 0.1, 1, 10, 100],\n",
    "    'model__penalty': ['l2'],\n",
    "    'model__solver': ['lbfgs', 'saga']\n",
    "}\n",
    "\n",
    "model_with_gridsearch(df, LogisticRegression(max_iter=1000, random_state=42), param_grid_lr)\n",
    "\n",
    "\n",
    "\n",
    "    #########################################################\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "param_grid_rf = {\n",
    "    'model__n_estimators': [100, 200],\n",
    "    'model__max_depth': [None, 10, 20],\n",
    "    'model__min_samples_split': [2, 5]\n",
    "}\n",
    "\n",
    "custom_pipeline_rf = [\n",
    "    ('outlier_handler', OutlierCapperOrRemover(strategy='cap', factor=1.5)),\n",
    "    ('imputer', KNNImputer(n_neighbors=5)),\n",
    "    ('smote', SMOTE(random_state=42))  # no scaler for trees\n",
    "]\n",
    "\n",
    "model_with_gridsearch(\n",
    "    data=df,\n",
    "    model=RandomForestClassifier(random_state=42),\n",
    "    param_grid=param_grid_rf,\n",
    "    pipeline_steps=custom_pipeline_rf,\n",
    "    scoring='roc_auc'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fece9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to detect outlier rows using IQR\n",
    "def detect_outliers_iqr(data, columns):\n",
    "    outlier_indices = set()\n",
    "    for col in columns:\n",
    "        Q1 = np.percentile(data[col].dropna(), 25)\n",
    "        Q3 = np.percentile(data[col].dropna(), 75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        outliers = data[(data[col] < lower_bound) | (data[col] > upper_bound)].index\n",
    "        outlier_indices.update(outliers)\n",
    "    return list(outlier_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e647d71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cap_outliers_iqr(data, columns):\n",
    "    capped_data = data.copy()\n",
    "    for col in columns:\n",
    "        Q1 = np.percentile(capped_data[col].dropna(), 25)\n",
    "        Q3 = np.percentile(capped_data[col].dropna(), 75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        # Cap the values outside the bounds\n",
    "        capped_data[col] = np.where(\n",
    "            capped_data[col] < lower_bound, lower_bound,\n",
    "            np.where(capped_data[col] > upper_bound, upper_bound, capped_data[col])\n",
    "        )\n",
    "    return capped_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3e8dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def manual_correlation_analysis(X, threshold=0.9, plot=True):\n",
    "    \"\"\"\n",
    "    Analyze feature-feature correlations and suggest features to drop manually.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : pandas DataFrame\n",
    "        Input feature matrix (without target variable).\n",
    "    \n",
    "    threshold : float, optional (default=0.9)\n",
    "        Correlation value above which features are considered too correlated.\n",
    "\n",
    "    plot : bool, optional (default=True)\n",
    "        Whether to plot the correlation heatmap.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    to_drop : list\n",
    "        List of feature names suggested to be dropped.\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Correlation matrix\n",
    "    corr_matrix = X.corr().abs()\n",
    "\n",
    "    # Step 2: Plot heatmap if needed\n",
    "    if plot:\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        sns.heatmap(corr_matrix, cmap='coolwarm', annot=False)\n",
    "        plt.title('Feature Correlation Matrix')\n",
    "        plt.show()\n",
    "\n",
    "    # Step 3: Get the upper triangle\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "\n",
    "    # Step 4: Find features to drop\n",
    "    to_drop = [column for column in upper.columns if any(upper[column] > threshold)]\n",
    "\n",
    "    print(f\"\\nFeatures suggested for dropping (correlation > {threshold}):\\n{to_drop}\")\n",
    "\n",
    "    return to_drop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175a1843",
   "metadata": {},
   "outputs": [],
   "source": [
    "######## DO NOT USE ############### Learning Purpose Only ################\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "class CorrelationFilterMixed(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    A transformer to drop highly correlated features, handling numeric and categorical features separately.\n",
    "    Keeps the feature more correlated with the target.\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    threshold: float (default=0.9)\n",
    "        Threshold for correlation to consider dropping.\n",
    "    categorical_cols: list (optional)\n",
    "        List of column names that are categorical.\n",
    "    \"\"\"\n",
    "    def __init__(self, threshold=0.9, categorical_cols=None):\n",
    "        self.threshold = threshold\n",
    "        self.categorical_cols = categorical_cols\n",
    "        self.features_to_drop_ = None\n",
    "        self.target_correlation_ = None\n",
    "\n",
    "    def _cramers_v(self, x, y):\n",
    "        \"\"\"Calculate CramÃ©r's V statistic for categorical-categorical association.\"\"\"\n",
    "        confusion_matrix = pd.crosstab(x, y)\n",
    "        chi2 = chi2_contingency(confusion_matrix)[0]\n",
    "        n = confusion_matrix.sum().sum()\n",
    "        phi2 = chi2 / n\n",
    "        r, k = confusion_matrix.shape\n",
    "        phi2corr = max(0, phi2 - ((k-1)*(r-1)) / (n-1))    \n",
    "        rcorr = r - ((r-1)**2)/(n-1)\n",
    "        kcorr = k - ((k-1)**2)/(n-1)\n",
    "        return np.sqrt(phi2corr / min((kcorr-1), (rcorr-1)))\n",
    "\n",
    "    def _calculate_target_correlation(self, X, y):\n",
    "        target_corr = {}\n",
    "        for col in X.columns:\n",
    "            if col in self.categorical_cols:\n",
    "                target_corr[col] = np.abs(self._cramers_v(X[col], y))\n",
    "            else:\n",
    "                target_corr[col] = np.abs(np.corrcoef(X[col], y)[0, 1])\n",
    "        return target_corr\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Identify features to drop by checking feature-feature and feature-target correlations.\n",
    "        \"\"\"\n",
    "        if not isinstance(X, pd.DataFrame):\n",
    "            raise ValueError(\"Input must be a pandas DataFrame\")\n",
    "        \n",
    "        X = X.copy()\n",
    "        y = pd.Series(y)\n",
    "        \n",
    "        num_cols = [col for col in X.columns if col not in self.categorical_cols]\n",
    "        \n",
    "        features_to_drop = set()\n",
    "\n",
    "        # 1. Handle numeric features\n",
    "        if num_cols:\n",
    "            corr_matrix = X[num_cols].corr().abs()\n",
    "            upper_triangle = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "            for col in upper_triangle.columns:\n",
    "                for row in upper_triangle.index:\n",
    "                    if upper_triangle.loc[row, col] > self.threshold:\n",
    "                        features_to_drop.add((row, col))\n",
    "        \n",
    "        # 2. Handle categorical features\n",
    "        if self.categorical_cols:\n",
    "            for i, col1 in enumerate(self.categorical_cols):\n",
    "                for col2 in self.categorical_cols[i+1:]:\n",
    "                    v = self._cramers_v(X[col1], X[col2])\n",
    "                    if v > self.threshold:\n",
    "                        features_to_drop.add((col1, col2))\n",
    "\n",
    "        # 3. Now decide which feature to drop based on correlation with target\n",
    "        target_corr = self._calculate_target_correlation(X, y)\n",
    "        self.target_correlation_ = target_corr\n",
    "\n",
    "        drop_list = set()\n",
    "        for f1, f2 in features_to_drop:\n",
    "            if target_corr[f1] < target_corr[f2]:\n",
    "                drop_list.add(f1)\n",
    "            else:\n",
    "                drop_list.add(f2)\n",
    "\n",
    "        self.features_to_drop_ = list(drop_list)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Drop the identified features from the dataset.\n",
    "        \"\"\"\n",
    "        X = X.copy()\n",
    "        if self.features_to_drop_:\n",
    "            X = X.drop(columns=self.features_to_drop_, errors='ignore')\n",
    "        return X\n",
    "    \n",
    "\n",
    "##########################################################\n",
    "\n",
    "\n",
    "# Assume we know which columns are categorical\n",
    "categorical_columns = ['currentSmoker', 'prevalentStroke', 'prevalentHyp', 'diabetes', 'male']\n",
    "\n",
    "# Initialize upgraded filter\n",
    "corr_filter = CorrelationFilterMixed(threshold=0.9, categorical_cols=categorical_columns)\n",
    "\n",
    "# Fit and transform\n",
    "X_reduced = corr_filter.fit_transform(X_train, y_train)\n",
    "\n",
    "print(\"Features dropped:\", corr_filter.features_to_drop_)\n",
    "\n",
    "\n",
    "\n",
    "##########################################################\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import KNNImputer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('corr_filter', CorrelationFilterMixed(threshold=0.9, categorical_cols=categorical_columns)),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('imputer', KNNImputer(n_neighbors=5)),\n",
    "    ('smote', SMOTE(random_state=42)),\n",
    "    ('model', LogisticRegression(max_iter=1000, random_state=42))\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd4db37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class OutlierCapperOrRemover(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Scikit-learn compatible transformer to cap or remove outliers based on IQR method.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    strategy : str, optional (default='cap')\n",
    "        Strategy to handle outliers: \n",
    "        'cap' - clip values to IQR bounds, \n",
    "        'remove' - remove rows containing outliers.\n",
    "    \n",
    "    factor : float, optional (default=1.5)\n",
    "        The multiplication factor for IQR to define outlier boundaries (standard = 1.5).\n",
    "    \n",
    "    columns : list, optional (default=None)\n",
    "        List of column names to apply outlier handling on. \n",
    "        If None, will apply to all numeric columns.\n",
    "    \"\"\"\n",
    "    def __init__(self, strategy='cap', factor=1.5, columns=None):\n",
    "        self.strategy = strategy\n",
    "        self.factor = factor\n",
    "        self.columns = columns\n",
    "        self.bounds_ = {}\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Learn the IQR bounds for each feature.\n",
    "        \"\"\"\n",
    "        X = X.copy()\n",
    "        if self.columns is None:\n",
    "            self.columns = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        \n",
    "        for col in self.columns:\n",
    "            Q1 = np.percentile(X[col].dropna(), 25)\n",
    "            Q3 = np.percentile(X[col].dropna(), 75)\n",
    "            IQR = Q3 - Q1\n",
    "            lower_bound = Q1 - self.factor * IQR\n",
    "            upper_bound = Q3 + self.factor * IQR\n",
    "            self.bounds_[col] = (lower_bound, upper_bound)\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Apply capping or removal based on learned bounds.\n",
    "        \"\"\"\n",
    "        X = X.copy()\n",
    "        \n",
    "        if self.strategy == 'cap':\n",
    "            for col, (lower, upper) in self.bounds_.items():\n",
    "                X[col] = np.clip(X[col], lower, upper)\n",
    "            return X\n",
    "\n",
    "        elif self.strategy == 'remove':\n",
    "            for col, (lower, upper) in self.bounds_.items():\n",
    "                X = X[(X[col] >= lower) & (X[col] <= upper)]\n",
    "            X = X.reset_index(drop=True)\n",
    "            return X\n",
    "\n",
    "        else:\n",
    "            raise ValueError(\"Strategy must be either 'cap' or 'remove'.\")\n",
    "    \n",
    "    \n",
    "    ##########################################################\n",
    "\n",
    "\n",
    "# Example with Outlier Capping\n",
    "outlier_handler = OutlierCapperOrRemover(strategy='cap', factor=1.5)\n",
    "X_transformed = outlier_handler.fit_transform(X_train)\n",
    "\n",
    "# Example with Outlier Removal\n",
    "outlier_handler = OutlierCapperOrRemover(strategy='remove', factor=1.5)\n",
    "X_transformed = outlier_handler.fit_transform(X_train)\n",
    "\n",
    "\n",
    "   ##########################################################\n",
    "\n",
    "\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('outlier_handler', OutlierCapperOrRemover(strategy='cap', factor=1.5)),\n",
    "    ('corr_filter', CorrelationFilterMixed(threshold=0.9, categorical_cols=['categorical_columns'])),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('imputer', KNNImputer(n_neighbors=5)),\n",
    "    ('smote', SMOTE(random_state=42)),\n",
    "    ('model', LogisticRegression(max_iter=1000, random_state=42))\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c2968c",
   "metadata": {},
   "source": [
    "<img src=\"Images/Pipeline_Order.png\" width=\"400\" height=\"600\" alt text=\"Pipeline_Order.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94575cee",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
